# CLASIFICACION_MALWARE_UIDE_2024

## Grupo 4 - Integrantes
* EDGAR DANIEL SULCA NARANJO
* GABRIELA ESTEFANIA CHILIQUINGA JIMENEZ
* KAREN LISSETTE PLAZA JIMENEZ
* MARJURY LISBETH DIAZ HARO
* JARIS SURYA AIZPRUA BARRIOS

## 1) Introducción

En el contexto actual, la ciberseguridad es un campo crítico con creciente relevancia. Este ejercicio tiene como objetivo construir un clasificador de malware utilizando un dataset personalizado creado específicamente para este propósito.

## 2) Preparación del dataset

2.1) Cargar el dataset: Leeremos el archivo CSV para ver las primeras filas y entender la estructura de los datos.
2.2) Exploración inicial: Identificaremos el número de filas, columnas, tipos de datos de las columnas, y si hay valores nulos.
2.3) Preprocesamiento: Trataremos los valores nulos o inconsistentes.
2.4) Reducción de dimensionalidad: Evaluaremos la necesidad y aplicaremos técnicas de reducción si es necesario.
2.5) División del dataset: Separaremos los datos en conjuntos de entrenamiento, validación y prueba.

### 2.1) Carga del dataset
El dataset contiene 532 columnas, incluyendo una columna de etiquetas (Label) que indica si el archivo ejecutable es malicioso o no ("non-malicious" para benigno) y 531 características numéricas (F_1 a F_531) que representan las características híbridas extraídas de los archivos ejecutables.

### 2.2) Exploración inicial
El dataset consta de 373 filas y 532 columnas. Todos los atributos, excepto la columna de etiquetas (Label), son de tipo numérico (int64), lo que sugiere que las características han sido preprocesadas para representar valores enteros. No se han detectado valores nulos en ninguna de las columnas.

### 2.3) Preprocesamiento
Dado que no hay valores nulos, podemos proceder directamente a evaluar la necesidad de reducción de dimensionalidad antes de dividir el dataset.

### 2.4) Reducción de dimensiones
Se realiza un análisis rápido de la varianza de las características para identificar si hay variables con poca varianza que podrían ser candidatas a eliminarse, lo cual es un paso preliminar antes de considerar técnicas más avanzadas de reducción de dimensionalidad como PCA (Análisis de Componentes Principales).

#### 2.4.1) Normalización de los datos
Antes de aplicar PCA, es crucial normalizar los datos para que cada característica tenga media 0 y varianza 1. Esto es importante porque PCA es sensible a las escalas de las variables. La normalización asegura que las características con varianzas más grandes no dominen los resultados.

#### 2.4.2) Aplicación de PCA
PCA se aplica a los datos normalizados para identificar las direcciones (componentes principales) que maximizan la varianza en los datos. Esto se hace calculando los vectores propios y valores propios de la matriz de covarianza de los datos normalizados o mediante la descomposición en valores singulares (SVD) de la matriz de datos.

#### 2.4.3) Selección de Componentes
Después de aplicar un filtro de varianza con un umbral diseñado para eliminar características con baja varianza (características que juntas explican al menos el 95% de la varianza total), encontramos que solo 64 de las 531 características tienen una varianza no baja. Esto sugiere que una gran parte de las características podrían no contribuir significativamente a la capacidad predictiva del modelo y que la reducción de dimensionalidad podría ser beneficiosa para simplificar el modelo y potencialmente mejorar el rendimiento.
El umbral del 95% en el contexto de PCA (Análisis de Componentes Principales) es una elección comúnmente utilizada pero arbitraria que busca equilibrar la reducción de dimensionalidad con la retención de información.

#### 2.4.4) Transformación de los Datos
Los datos originales se proyectan en el espacio de los 64 componentes principales seleccionados. Esto se realiza multiplicando la matriz de datos original (normalizada) por la matriz de vectores propios correspondientes a los componentes seleccionados. El resultado es un nuevo conjunto de datos de dimensiones reducidas (64 características) que conserva el 95% de la varianza original.

### 2.5) División del dataset
El siguiente paso es dividir el dataset en conjuntos de entrenamiento, validación y prueba. Esta división es crucial para entrenar modelos de machine learning, validar su rendimiento y finalmente probar su capacidad para generalizar a nuevos datos. La división típica es 70% para entrenamiento, 15% para validación, y 15% para prueba, aunque estas proporciones pueden variar según el tamaño y la especificidad del dataset.

## 3) Selección de técnicas de aprendizaje automático

### 3.1) Técnicas de aprendizaje automático para la clasificación de Malware
La clasificación de malware es un problema de clasificación binaria donde el objetivo es distinguir entre software benigno y malicioso. Varias técnicas de aprendizaje automático son adecuadas para esta tarea, incluyendo:

* Árboles de Decisión y Random Forest: Son métodos potentes y fáciles de interpretar que pueden manejar características categóricas y numéricas. Random Forest, en particular, es efectivo en conjuntos de datos de alta dimensionalidad y puede manejar el desbalance de clases mediante el ajuste de pesos de clase o mediante técnicas de muestreo.
* Gradient Boosting Machines (GBM): Técnicas como XGBoost, LightGBM, y CatBoost son muy populares por su eficacia en competiciones de ciencia de datos. Estos métodos construyen modelos de manera secuencial para corregir los errores de los modelos anteriores y pueden manejar el desbalance de clases ajustando los pesos de clase o usando técnicas de muestreo.
* Support Vector Machines (SVM): SVM puede ser muy efectivo para conjuntos de datos de alta dimensionalidad y puede configurarse para manejar el desbalance de clases mediante el ajuste de los parámetros de penalización.
* Redes Neuronales Artificiales: Las redes neuronales profundas pueden capturar complejas relaciones no lineales en los datos. Para el desbalance de clases, se pueden ajustar los pesos de las clases o utilizar técnicas de muestreo para entrenar modelos más equilibrados.

### 3.2) Selección de técnicas y justificación
Para este caso, se recomienda utilizar Random Forest y XGBoost por las siguientes razones:

* Random Forest: Es robusto frente al desbalance de clases y puede manejar una alta dimensionalidad sin la necesidad de una selección extensiva de características. También proporciona una buena interpretabilidad a través de la importancia de las características.
* XGBoost: Es conocido por su rendimiento y eficiencia en problemas de clasificación. XGBoost ofrece formas integradas para manejar el desbalance de clases mediante el ajuste de los parámetros scale_pos_weight y max_delta_step, lo que puede mejorar significativamente el rendimiento en conjuntos de datos desbalanceados.

